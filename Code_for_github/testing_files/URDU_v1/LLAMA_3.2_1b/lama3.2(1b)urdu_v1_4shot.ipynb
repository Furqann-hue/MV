{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbde9da5-acf3-4362-bb2e-338c154d8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4624f9a-4c47-48ba-880e-726adeecc79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "CSV_PATH = r\"C:\\Users\\stdFurqan\\Downloads\\urdu_v1\\test.csv\" \n",
    "TEXT_COL = \"Cleaned_Tweet\"\n",
    "LABEL_COL = \"Class\"\n",
    "LABELS = [\"P\", \"N\"]\n",
    "SEED = 20\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seed(seed=20):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14cca0f3-6495-4fd9-a775-6aaaa4fb4077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Loading model from C:\\Users\\stdFurqan\\Downloads\\lama_models_download\\LAMA_3.2(1b) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The module name LAMA_3_dot_2(1b) (originally LAMA_3.2(1b)) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully from local directory!\n"
     ]
    }
   ],
   "source": [
    "# âœ… Local path where model is stored\n",
    "MODEL_PATH = r\"C:\\Users\\stdFurqan\\Downloads\\lama_models_download\\LAMA_3.2(1b)\"\n",
    "\n",
    "print(f\"ğŸ§  Loading model from {MODEL_PATH} ...\")\n",
    "\n",
    "# âœ… Load tokenizer from local folder\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# âœ… Load model from local folder\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,   # âœ… best for 40-series\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model loaded successfully from local directory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f991566-6386-42a9-8db2-a3701d28306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cleaned_Tweet', 'Class']\n"
     ]
    }
   ],
   "source": [
    "# --- Load dataset ---\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "# âœ… Clean up any extra spaces or tabs in column names\n",
    "df.columns = df.columns.str.strip()\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae303ed-0880-47d6-8bda-3d52bf925761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           P     0.7188    0.1597    0.2614       144\n",
      "           N     0.3333    0.0067    0.0131       150\n",
      "\n",
      "   micro avg     0.6857    0.0816    0.1459       294\n",
      "   macro avg     0.5260    0.0832    0.1372       294\n",
      "weighted avg     0.5221    0.0816    0.1347       294\n",
      "\n",
      "Accuracy: 0.0816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "texts = df[TEXT_COL].astype(str).tolist()\n",
    "true_labels = df[LABEL_COL].astype(str).tolist()\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"You are an intelligent Urdu language assistant.\"\n",
    "    \"Classify the following news headline into one of these categories: Positive, or Negative based on the examples below.\\n\\n\"\n",
    "    \n",
    "    \"=== Positive Examples ===\\n\"\n",
    "    \"1 Text: Ú©Ù„Ø§Ø³Ú© ÙˆÛŒÚˆÛŒÙˆ Ú¯ÛŒÙ… Ú©ÛŒ Ø´Ú©Ù„ Ù…ÛŒÚº Ø¨Ù†Ø§ÛŒØ§ Ú¯ÛŒØ§ Ù…Ù†ÙØ±Ø¯ Ú©ÛŒÚ© Ù†ÛŒÙˆÛŒØ§Ø±Ú© Ú©ÛŒÚ© Ø®ÙˆØ§Û Ø³Ø§Ù„Ú¯Ø±Û Ú©Ø§ ÛÙˆÛŒØ§ Ú©Ø³ÛŒ Ø§ÙˆØ±\\n   Label: Positive\\n\"\n",
    "    \"2 Text: Ù…Ø¬Ø§ÛØ¯ Ø§Ø¹Ø¸Ù… Ù…ÙˆÙ„Ø§Ù†Ø§ ØºÙ„Ø§Ù… Ø§Ø¹Ø¸Ù… Ù†Û’ Ø§Ù¾Ù†ÛŒ Ø¬ÙˆØ§Ù†ÛŒ Ø³Û’ Ø¨Ú‘Ú¾Ø§ Ù¾Û’ ØªÚ© ØµØ¨Ø± Ø¹Ø²Ù…ÛŒØª Ø§ÙˆØ± Ù…Ø³Ù„Ø³Ù„ Ø¬Ø¯ÙˆØ¬ÛØ¯ Ú©ÛŒ ØªØ§Ø±ÛŒØ® Ø±Ù‚Ù… Ú©Ø± Ø¯ÛŒ Ø§Ø³\\n   Label: Positive\\n\"\n",
    "    \"3 Text: Ù…ÛŒÚº Ø¢Ù¾Ú©ÛŒ Ø¨Ø§Øª Ø³Û’ Ø§ØªÙØ§Ù‚ Ú©Ø±ØªØ§ ÛÙˆÚº Ù„ÛŒÚ©Ù† Ù…ÛŒÚº Ø³Ø¨ Ú©ÛŒ Ø±Ø§Ø¦Û’ Ù„ÛŒÙ†Ø§ Ø§ÙˆØ± Ø³Ù†Ù†Ø§ Ù¾Ø³Ù†Ø¯ Ú©Ø±ØªØ§ ÛÙˆÚº Ø§ÛŒÚ© Ø¹Ø§Ø¯ÛŒ Ø¨Ú¾Ø§Ø¦ÛŒ ÛÛŒÚº Ú©Ø¨Ú¾ÛŒ Ú©Ø§Ù… Ú©ÛŒ Ø¨Ø§Øª Ù†Ø¦ Ú©ÛŒ Ù¾ Ø³Ù†ØªØ§\\n   Label: Positive\\n\"\n",
    "    \"4 Text: ÙˆÛŒÙ†Ø§ Ù…Ù„Ú© Ú©Ø§ Ø¨ÛŒÙ¹Ø§ Ø³ÙˆØ´Ù„ Ù…ÛŒÚˆÛŒØ§ Ù¾Ø± Ù…Ù‚Ø¨ÙˆÙ„ÛŒØª Ù…ÛŒÚº Ø´Ù„Ù¾Ø§ Ø´ÛŒÙ¹Ú¾ÛŒ Ú©Û’ Ø¨ÛŒÙ¹Û’ Ø³Û’ Ø¢Ú¯Û’ Ú©Ø±Ø§Ú†ÛŒ ÙˆÛŒÙ†Ø§ Ù…Ù„Ú© Ú©Û’ Ø¨ÛŒÙ¹Û’ Ø§Ø¨Ø±Ø§Ù… Ø®Ø§Ù†\\n   Label: Positive\\n\\n\"\n",
    "\n",
    "    \"=== Negative Examples ===\\n\"\n",
    "    \"5 Text: Ø³ÛŒØ§Ù„Ú©ÙˆÙ¹ ÛÙ†Ø¯ÙˆØ³ØªØ§Ù†ÛŒ ÙÙˆØ±Ø³Ø² Ú©ÛŒ Ø¨Ù„Ø§ Ø§Ø´ØªØ¹Ø§Ù„ ÙØ§Ø¦Ø±Ù†Ú¯ Ù¾Ø§Ú©Ø³ØªØ§Ù† Ú©Û’ ØµÙˆØ¨Û Ù¾Ù†Ø¬Ø§Ø¨ Ú©Û’ Ø´ÛØ± Ø³ÛŒØ§Ù„Ú©ÙˆÙ¹ Ú©ÛŒ ÙˆØ±Ú©Ù†Ú¯\\n   Label: Negative\\n\"\n",
    "    \"6 Text: Ú©Ø±Ø§Ú†ÛŒ Ù…ÛŒÚº ØªÙ…Ø§Ù… Ù†ÛŒÙˆØ² Ú†ÛŒÙ†Ù„ Ú©ÛŒ Ø¨Ù†Ø¯Ø´Ø§ÛŒÙ… Ú©ÛŒÙˆ Ø§ÛŒÙ… Ø§ÙˆØ± Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…Ø³Ù„Ù… Ù„ÛŒÚ¯ Ø²Ù†Ø¯Û Ø¨Ø§Ø¯Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…Ù„Ú© ÛÛ’ Ø´ÛÙ†Ø´Ø§ÛÙˆÚº Ú©Ø§ Ø¨Ø¯Ù…Ø¹Ø§Ø´ÙˆÚº Ú©Ø§\\n   Label: Negative\\n\"\n",
    "    \"7 Text: ï»‹ïºï»Ÿï»¤ï¯½ ïºïº³ïº˜ï»Œï»¤ïºïº­ ïºï»£ïº®ï¯¾ï®‘ïº ï»­ ïºïº³ïº®ïºïº‹ï¯¿ï» ïºï»­ïº­ïº ï»§ï®‘ï®¯ ï»§ï»¤ï® ïº§ï»®ïºïº­ ïº³ï»Œï»®ïº©ï¯¼ ï»‹ïº®ïº ï®ï®¯ ïº³ï¯¿ï»¨ï®¯ï»£ï¯¿ï®Ÿ ïº§ï»¨ïº ïº® ï®ï¯½ ï»£ïºï»§ï»¨ïºª ï»£ïºï®¦ ï»£ïº¤ïº®ï»¡ ïº»ï»”ïº®ïºï»­ïº­ ïº­ïº‘ï¯¿ï»Š ïºï»»ï»­ï»\\n   Label: Negative\\n\"\n",
    "    \"8 Text: Ø¬Ù†Ø§Ø¨ Ø§Ø³Ù¾ÛŒÚ©Ø± Ø¢Ø¬ Ø³Û’ Ù†ÙˆÙ† Ù„ÛŒÚ¯ Ú©Ùˆ Ù…Ù†Ø§ÙÙ‚ Ù„ÛŒÚ¯ Ú©Ø§ Ù†Ø§Ù… Ø¯ÛŒØ§ Ø¬Ø§Ø¦Û’ Ú©ÛŒÙˆÙ†Ú©Û Ù†ÙˆÙ† Ù„ÛŒÚ¯ Ù†Û’ Ø³Ú† ØªÙˆ Ú©Ø¨Ú¾ÛŒ Ø¨ÙˆÙ„Ù†Ø§ Ù†ÛÛŒÚº\\n   Label: Negative\\n\\n\" \n",
    "\n",
    "    \"Reply with only one word: P, or N.\\n\"\n",
    "    \"Now classify the following text in the same way.\\n\\n\"\n",
    "    \"Headline: {text}\\n\\nLabel:\"\n",
    ")\n",
    "\n",
    "\n",
    "pattern = re.compile(r\"\\b(P|N)\\b\", re.IGNORECASE)\n",
    "\n",
    "def extract_label(output):\n",
    "    m = pattern.search(output)\n",
    "    return m.group(1).capitalize() if m else \"Unknown\"\n",
    "\n",
    "pred_labels = []\n",
    "batch_size = 4  # smaller batch to fit in T4 VRAM\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i+batch_size]\n",
    "    prompts = [PROMPT_TEMPLATE.format(text=t) for t in batch]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=6,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            top_p=1.0,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for prompt, full_out in zip(prompts, decoded):\n",
    "        gen = full_out[len(prompt):].strip() if full_out.startswith(prompt) else full_out\n",
    "        pred_labels.append(extract_label(gen))\n",
    "\n",
    "# --- Evaluation ---\n",
    "report = classification_report(true_labels, pred_labels, labels=LABELS, digits=4, zero_division=0)\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "print(\"\\nğŸ“Š Classification Report:\\n\")\n",
    "print(report)\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700d37f-1b2b-44b1-8998-47c6ff2801ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
