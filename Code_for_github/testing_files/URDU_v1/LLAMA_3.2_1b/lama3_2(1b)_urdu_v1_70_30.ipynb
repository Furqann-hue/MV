{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ce486-e63f-4cd8-9083-23c0e7f796df",
   "metadata": {
    "id": "b58ce486-e63f-4cd8-9083-23c0e7f796df"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers==4.44.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953a5b1-5db5-4b10-ad5a-efc4c37a7a93",
   "metadata": {
    "id": "b953a5b1-5db5-4b10-ad5a-efc4c37a7a93"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a8ef1-f2c9-4bb2-84e1-67094ca94a09",
   "metadata": {
    "id": "ec4a8ef1-f2c9-4bb2-84e1-67094ca94a09"
   },
   "outputs": [],
   "source": [
    "# !pip install \"huggingface_hub[hf_xet]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb25813-b985-4b98-bf53-ac88c22b800d",
   "metadata": {
    "id": "bdb25813-b985-4b98-bf53-ac88c22b800d"
   },
   "outputs": [],
   "source": [
    "# !pip install -U transformers accelerate huggingface_hub safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad06cc63-3d3d-478c-b5c2-004c718b745a",
   "metadata": {
    "id": "ad06cc63-3d3d-478c-b5c2-004c718b745a"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy pandas matplotlib scikit-learn torch torchvision torchaudio datasets tqdm jupyter ipykernel seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b40753-db1a-42a0-8bf8-e227d01edf0d",
   "metadata": {
    "id": "d5b40753-db1a-42a0-8bf8-e227d01edf0d"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login, HfApi\n",
    "# login(token=\"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\")\n",
    "\n",
    "# api = HfApi()\n",
    "# api.whoami()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d95916-fe2a-4e7b-b6e7-3fd518c294d9",
   "metadata": {
    "id": "a3d95916-fe2a-4e7b-b6e7-3fd518c294d9"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "# api = HfApi()\n",
    "# api.model_info(\"meta-llama/Llama-3.2-1B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efccbaed-589a-4e93-a64a-6b31aa2fcce1",
   "metadata": {
    "id": "efccbaed-589a-4e93-a64a-6b31aa2fcce1"
   },
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35113b6d-a1fa-4c85-a326-ec8f28d486a1",
   "metadata": {
    "id": "35113b6d-a1fa-4c85-a326-ec8f28d486a1"
   },
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8bf9fb-70d0-4821-8cd4-04e471cc29c1",
   "metadata": {
    "id": "1a8bf9fb-70d0-4821-8cd4-04e471cc29c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7284f16-570b-48f5-856e-95467fa0f5ca",
   "metadata": {
    "id": "a7284f16-570b-48f5-856e-95467fa0f5ca"
   },
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# üß† Fine-tune LLaMA 3.2-1B on Pashto Sentiment Dataset (Local)\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "from huggingface_hub import snapshot_download\n",
    "# from tqdm.auto import tqdm  # instead of tqdm.notebook\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e8f55b-ff29-4624-a2f7-10fe46080c78",
   "metadata": {
    "id": "e1e8f55b-ff29-4624-a2f7-10fe46080c78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded successfully!\n",
      "Train size: 685 | Test size: 294\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "TRAIN_PATH = r\"C:\\Users\\stdFurqan\\Downloads\\urdu_v1\\70_urdu_v1.csv\"\n",
    "TEST_PATH  = r\"C:\\Users\\stdFurqan\\Downloads\\urdu_v1\\test.csv\"\n",
    "TEXT_COL = \"Cleaned_Tweet\"\n",
    "LABEL_COL = \"Class\"\n",
    "LABELS = [\"P\", \"N\"]\n",
    "SEED = 20\n",
    "EPOCHS = 5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "def set_seed(seed=20):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Robust CSV loader\n",
    "# -----------------------------\n",
    "def safe_read_csv(path):\n",
    "    encodings = ['utf-8', 'latin1', 'ISO-8859-1', 'cp1252']\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, on_bad_lines='skip')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed with {enc}: {e}\")\n",
    "    raise ValueError(f\"‚ùå Could not read file: {path}\")\n",
    "\n",
    "train_df = safe_read_csv(TRAIN_PATH)\n",
    "test_df  = safe_read_csv(TEST_PATH)\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"Train size: {len(train_df)} | Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc06552-fe71-4b70-9cfa-dfccbddbf493",
   "metadata": {
    "id": "9bc06552-fe71-4b70-9cfa-dfccbddbf493"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# HF_TOKEN = \"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\"  # replace with your actual token\n",
    "\n",
    "# local_dir = r\"C:\\Users\\stdFurqan\\Downloads\\lama_models_download\\LAMA_3.2(1b)\"\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "#     local_dir=local_dir,\n",
    "#     token=HF_TOKEN,\n",
    "#     local_dir_use_symlinks=False\n",
    "# )\n",
    "\n",
    "# print(\"‚úÖ Model successfully downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17327b79-b83d-4fcf-9290-265df5195c9b",
   "metadata": {
    "id": "17327b79-b83d-4fcf-9290-265df5195c9b"
   },
   "outputs": [],
   "source": [
    "# # ‚úÖ Local directory where you want to store the model\n",
    "# local_dir = r\"C:\\Users\\stdFurqan\\Downloads\\lama_models_download\\LAMA_3.2(1b)\"\n",
    "\n",
    "# # ‚úÖ Download the entire model snapshot\n",
    "# snapshot_download(\n",
    "#     repo_id=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "#     local_dir=local_dir,\n",
    "#     token=HF_TOKEN,\n",
    "#     local_dir_use_symlinks=False  # safer for Windows (no symlink issues)\n",
    "# )\n",
    "\n",
    "# print(f\"‚úÖ Model successfully downloaded to: {local_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74ec5bf-e6dd-4913-9420-0f8711f79970",
   "metadata": {
    "id": "c74ec5bf-e6dd-4913-9420-0f8711f79970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loading model from C:\\Users\\stdFurqan\\Downloads\\lama_models_download\\LAMA_3.2(1b) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The module name LAMA_3_dot_2(1b) (originally LAMA_3.2(1b)) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully from local directory!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Local path where model is stored\n",
    "MODEL_PATH = r\"C:\\Users\\stdFurqan\\Downloads\\lama_models_download\\LAMA_3.2(1b)\"\n",
    "# MODEL_PATH = r\"C:\\Users\\stdFurqan\\Downloads\\lama_models_download\\LAMA_3.2(3b)\"\n",
    "\n",
    "\n",
    "print(f\"üß† Loading model from {MODEL_PATH} ...\")\n",
    "\n",
    "# ‚úÖ Load tokenizer from local folder\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# ‚úÖ Load model from local folder\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,   # ‚úÖ best for 40-series\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully from local directory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7ac96f-2529-43bc-9edd-4a533b5a8b68",
   "metadata": {
    "id": "9c7ac96f-2529-43bc-9edd-4a533b5a8b68"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt template\n",
    "# -----------------------------\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Classify the following news headline into one of these categories: Positive, or Negative.\\n\"\n",
    "    \"Reply with only one word: P, or N.\\n\\n\"\n",
    "    \"Headline: {text}\\n\\nLabel:\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def make_prompt(text, label=None):\n",
    "    if label is None:\n",
    "        return PROMPT_TEMPLATE.format(text=text)\n",
    "    return PROMPT_TEMPLATE.format(text=text) + \" \" + label\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset class\n",
    "# -----------------------------\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, is_train=True):\n",
    "        self.texts = df[TEXT_COL].astype(str).tolist()\n",
    "        self.labels = df[LABEL_COL].astype(str).tolist() if is_train else None\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        if self.is_train:\n",
    "            label = self.labels[idx]\n",
    "            prompt = make_prompt(text, label)\n",
    "            tokenized = self.tokenizer(prompt, truncation=True, padding=\"max_length\",\n",
    "                                       max_length=256, return_tensors=\"pt\")\n",
    "            tokenized = {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "            return tokenized\n",
    "        else:\n",
    "            prompt = make_prompt(text)\n",
    "            tokenized = self.tokenizer(prompt, truncation=True, padding=\"max_length\",\n",
    "                                       max_length=256, return_tensors=\"pt\")\n",
    "            return {k: v.squeeze(0) for k, v in tokenized.items()}\n",
    "\n",
    "train_dataset = SentimentDataset(train_df, tokenizer, is_train=True)\n",
    "test_dataset  = SentimentDataset(test_df, tokenizer, is_train=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f089a360-ed64-4931-a2d1-0ce48edd4d37",
   "metadata": {
    "id": "f089a360-ed64-4931-a2d1-0ce48edd4d37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdFurqan\\AppData\\Local\\Temp\\ipykernel_27156\\2155802087.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting fine-tuning ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='430' max='430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [430/430 02:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.314400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training finished! Now evaluating ...\n",
      "\n",
      "üìä Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           P     0.7067    0.7361    0.7211       144\n",
      "           N     0.7361    0.7067    0.7211       150\n",
      "\n",
      "    accuracy                         0.7211       294\n",
      "   macro avg     0.7214    0.7214    0.7211       294\n",
      "weighted avg     0.7217    0.7211    0.7211       294\n",
      "\n",
      "Accuracy: 0.7211\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Training configuration\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",              # ‚úÖ disable evaluation\n",
    "    save_strategy=\"epoch\",           # ‚úÖ save model each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,   # ‚úÖ fits 4080 SUPER\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    seed=SEED,\n",
    "    fp16=False,                      # ‚úÖ no AMP conflict\n",
    "    bf16=True,                       # ‚úÖ ideal for RTX 40-series\n",
    "    dataloader_num_workers=0,        # ‚úÖ Windows-safe (no multiprocessing issues)\n",
    "    load_best_model_at_end=False,    # ‚úÖ no eval ‚Üí don‚Äôt track best model\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",        # ‚úÖ matches your intent\n",
    "    # disable_tqdm=True,               # ‚úÖ optional: avoids frozen progress bars\n",
    "    report_to=[],                    # ‚úÖ no W&B/TensorBoard\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Training & Evaluation (Windows-safe entry point)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting fine-tuning ...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"\\n‚úÖ Training finished! Now evaluating ...\")\n",
    "\n",
    "    pattern = re.compile(r\"\\b(P|N)\\b\", re.IGNORECASE)\n",
    "\n",
    "    def extract_label(output):\n",
    "        m = pattern.search(output)\n",
    "        return m.group(1).capitalize() if m else \"Unknown\"\n",
    "\n",
    "    model.eval()\n",
    "    pred_labels = []\n",
    "    true_labels = test_df[LABEL_COL].astype(str).tolist()\n",
    "    texts = test_df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "    batch_size = 4\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        prompts = [make_prompt(t) for t in batch]\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=6,\n",
    "                do_sample=False,\n",
    "                top_p=1.0,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        for prompt, full_out in zip(prompts, decoded):\n",
    "            gen = full_out[len(prompt):].strip() if full_out.startswith(prompt) else full_out\n",
    "            pred_labels.append(extract_label(gen))\n",
    "\n",
    "    # -----------------------------\n",
    "    # Metrics\n",
    "    # -----------------------------\n",
    "    report = classification_report(true_labels, pred_labels, labels=LABELS, digits=4, zero_division=0)\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    print(\"\\nüìä Classification Report:\\n\")\n",
    "    print(report)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aaeb77-3501-47bf-b4b6-8f45e64c9fa3",
   "metadata": {
    "id": "c2aaeb77-3501-47bf-b4b6-8f45e64c9fa3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
